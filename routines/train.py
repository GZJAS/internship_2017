from __future__ import absolute_import
from __future__ import division
from __future__ import print_function

from six.moves import xrange
import time

import abc

import numpy as np
import tensorflow as tf

from data.color_depth import load_batch_color_depth, get_split_color_depth
from nets_base.arg_scope import nets_arg_scope

slim = tf.contrib.slim


class TrainAbstract(object):

    __metaclass__ = abc.ABCMeta

    @abc.abstractmethod
    def train(self, *args):
        pass

    @abc.abstractmethod
    def train_step(self, sess, train_op, global_step, *args):
        pass

    @abc.abstractmethod
    def get_data(self, tfrecord_dir, batch_size):
        pass

    @abc.abstractmethod
    def decide_used_data(self):
        pass

    @abc.abstractmethod
    def used_arg_scope(self, use_batch_norm, renorm):
        pass

    @abc.abstractmethod
    def compute(self, **kwargs):
        pass

    @abc.abstractmethod
    def get_total_loss(self):
        pass

    @abc.abstractmethod
    def get_learning_rate(self):
        pass

    @abc.abstractmethod
    def get_optimizer(self):
        pass

    @abc.abstractmethod
    def get_supervisor(self, log_dir, init_fn):
        pass

    @abc.abstractmethod
    def summary_log_info(self, sess):
        pass

    @property
    def default_trainable_scopes(self):
        return None

    def get_summary_op(self):
        pass

    def get_test_summary_op(self):
        pass

    def get_metric_op(self):
        pass

    def get_init_fn(self, checkpoint_dirs):
        """Returns a function run by the chief worker to
           warm-start the training."""
        return None

    def extra_initialization(self, sess):
        pass

    def test_log_info(self, sess, test_use_batch):
        pass

    def final_log_info(self, sess):
        pass


class Train(TrainAbstract):

    initial_learning_rate = 0.005
    lr_decay_steps = 100
    lr_decay_rate = 0.8

    def __init__(self, **kwargs):
        self.__dict__.update(kwargs)

    def train(self,
              tfrecord_dir,
              checkpoint_dirs,
              log_dir,
              number_of_steps=None,
              number_of_epochs=5,
              batch_size=24,
              save_summaries_steps=5,
              save_model_steps=250,
              do_test=True,
              trainable_scopes=None,
              use_default_trainable_scopes=True,
              use_batch_norm=True,
              renorm=False,
              test_use_batch=False,
              **kwargs):
        """Fine tune a pre-trained model using customized dataset.

        Args:
            tfrecord_dir: The directory that contains the tfreocrd files
              (which can be generated by data/convert_TFrecord.py)
            checkpoints_dir: The directory containing the checkpoint of
              the model to use
            log_dir: The directory to log event files and checkpoints
            number_of_steps: number of steps to run the training process
              (one step = one batch), if is None then number_of_epochs is used
            number_of_epochs: Number of epochs to run through the whole dataset
            batch_size: The batch size used to train and test (if any)
              save_summaries_steps: We save the summary every
              save_summaries_steps
            do_test: If True the test is done every save_summaries_steps and
              is shown on tensorboard
            trainable_scopes: The layers to train, if left None then all
            **kwargs: Arguments pass to the main structure/function
        """
        # Create the log directory if it doesn't exist
        if not tf.gfile.Exists(log_dir):
            tf.gfile.MakeDirs(log_dir)

        if (checkpoint_dirs is not None and
                not isinstance(checkpoint_dirs, (list, tuple))):
            checkpoint_dirs = [checkpoint_dirs]

        with tf.Graph().as_default():
            tf.logging.set_verbosity(tf.logging.INFO)

            # Read the data
            with tf.name_scope('Data_provider'):
                dataset = self.get_data(tfrecord_dir, batch_size)

            # Use number_of_epochs when number_of_steps is not given
            if number_of_steps is None:
                number_of_steps = int(np.ceil(
                    dataset.num_samples * number_of_epochs / batch_size))

            # Decide if we're training or not to use the right data
            self.training = tf.placeholder(tf.bool, shape=(), name='training')
            self.decide_used_data()

            # Decide if we use batch statstic or moving mean/variance
            # for the test (see test_log_info)
            self.batch_stat = tf.placeholder(
                tf.bool, shape=(), name='batch_stat')

            # Create the model, use the default arg scope to configure the
            # batch norm parameters
            with slim.arg_scope(self.used_arg_scope(use_batch_norm, renorm)):
                self.compute(**kwargs)

            # Specify the loss function
            # Create the global step for monitoring training
            # Specify the learning rate, optimizer and train op
            total_loss = self.get_total_loss()
            self.global_step = tf.train.get_or_create_global_step()
            self.get_learning_rate()
            optimizer = self.get_optimizer()

            # Decide what variables need to be trained
            if trainable_scopes is None:
                if (not use_default_trainable_scopes or
                        self.default_trainable_scopes is None):
                    variables_to_train = tf.trainable_variables()
                else:
                    variables_to_train = self.get_variables_to_train(
                        self.default_trainable_scopes)
            else:
                variables_to_train = \
                    self.get_variables_to_train(trainable_scopes)
            print(variables_to_train)

            # Create the training operation
            self.train_op = slim.learning.create_train_op(
                total_loss, optimizer,
                variables_to_train=variables_to_train)

            # The metrics to predict (may be omitted)
            self.get_metric_op()

            # Create some summaries to visualize the training process
            self.get_summary_op()
            self.get_test_summary_op()

            # Define the initialization function used by the supervisor
            if checkpoint_dirs is None:
                init_fn = None
            else:
                init_fn = self.get_init_fn(checkpoint_dirs)

            # Define the supervisor
            self.sv = self.get_supervisor(log_dir, init_fn)

            with self.sv.managed_session() as sess:

                # Finalize the initialization if necessary
                self.extra_initialization(sess)

                # Run the training process
                for step in xrange(number_of_steps):

                    # Save summries from time to time
                    if (step+1) % save_summaries_steps == 0:
                        self.summary_log_info(sess)
                        if do_test:
                            self.test_log_info(sess, test_use_batch)
                    else:
                        self.step_log_info(sess)

                    # Save the model from time to time
                    if (step+1) % save_model_steps == 0:
                        self.sv.saver.save(
                            sess, self.sv.save_path,
                            global_step=self.sv.global_step)

                # Finish training and save model to checkpoint
                self.final_log_info(sess)
                self.sv.saver.save(
                    sess, self.sv.save_path, global_step=self.sv.global_step)

    def train_step(self, sess, train_op, global_step, *args):
        tensors_to_run = [train_op, global_step]
        tensors_to_run.extend(args)

        start_time = time.time()
        tensor_values = sess.run(
            tensors_to_run,
            feed_dict={self.training: True, self.batch_stat: True})
        time_elapsed = time.time() - start_time

        total_loss = tensor_values[0]
        global_step_count = tensor_values[1]

        tf.logging.info(
            'global step %s: loss: %.4f (%.2f sec/step)',
            global_step_count, total_loss, time_elapsed)
        return tensor_values

    def used_arg_scope(self, use_batch_norm, renorm):
        return nets_arg_scope(
            is_training=self.batch_stat,
            use_batch_norm=use_batch_norm,
            renorm=renorm)

    def get_learning_rate(self):
        # Exponentially decaying learning rate
        self.learning_rate = tf.train.exponential_decay(
            learning_rate=self.initial_learning_rate,
            global_step=self.global_step,
            decay_steps=self.lr_decay_steps,
            decay_rate=self.lr_decay_rate, staircase=True)
        return self.learning_rate

    def get_optimizer(self):
        return tf.train.AdamOptimizer(learning_rate=self.learning_rate)

    def get_supervisor(self, log_dir, init_fn):
        return tf.train.Supervisor(
            logdir=log_dir, summary_op=None,
            init_fn=init_fn, save_model_secs=0)

    def step_log_info(self, sess):
        if hasattr(self, 'metric_op'):
            self.loss = self.train_step(
                sess, self.train_op, self.sv.global_step, self.metric_op)[0]
        else:
            self.loss = self.train_step(
                sess, self.train_op, self.sv.global_step)[0]

    def final_log_info(self, sess):
        tf.logging.info('Finished training. Final Loss: %s', self.loss)
        tf.logging.info('Saving model to disk now.')

    @staticmethod
    def get_variables_to_train(scopes):
        variables_to_train = []
        for scope in scopes:
            variables = tf.get_collection(
                tf.GraphKeys.TRAINABLE_VARIABLES, scope)
            variables_to_train.extend(variables)
        return variables_to_train

    @staticmethod
    def get_variables_to_restore(scopes=None, exclude=None):

        if scopes is not None:
            variables_to_restore = []
            for scope in scopes:
                variables = tf.get_collection(
                    tf.GraphKeys.MODEL_VARIABLES, scope)
                variables_to_restore.extend(variables)
        else:
            variables_to_restore = tf.model_variables()

        if exclude is not None:
            variables_to_restore_final = []
            for var in variables_to_restore:
                excluded = False
                for exclusion in exclude:
                    if var.op.name.startswith(exclusion):
                        excluded = True
                        break
                if not excluded:
                    variables_to_restore_final.append(var)
        else:
            variables_to_restore_final = variables_to_restore

        return variables_to_restore_final

    def get_batch_norm_summary(self):
        # Track moving mean and moving varaince
        try:
            last_moving_mean = [
                v for v in tf.model_variables()
                if v.op.name.endswith('moving_mean')][0]
            last_moving_variance = [
                v for v in tf.model_variables()
                if v.op.name.endswith('moving_variance')][0]
            tf.summary.histogram('batch_norm/last_layer/moving_mean',
                                 last_moving_mean)
            tf.summary.histogram('batch_norm/last_layer/moving_variance',
                                 last_moving_variance)
        except IndexError:
            tf.logging.info('No moiving mean or variance')


class TrainColorDepth(Train):

    def __init__(self, image_size=299,
                 color_channels=3, depth_channels=3, **kwargs):
        super(TrainColorDepth, self).__init__(**kwargs)
        self.image_size = image_size
        self.color_channels = color_channels
        self.depth_channels = depth_channels

    def get_data(self, tfrecord_dir, batch_size):

        self.dataset_train = get_split_color_depth(
            'train',
            tfrecord_dir,
            color_channels=self.color_channels,
            depth_channels=self.depth_channels)

        self.images_color_train, self.images_depth_train, self.labels_train = \
            load_batch_color_depth(
                self.dataset_train, height=self.image_size,
                width=self.image_size, batch_size=batch_size)

        self.dataset_test = get_split_color_depth(
            'validation',
            tfrecord_dir,
            color_channels=self.color_channels,
            depth_channels=self.depth_channels)

        self.images_color_test, self.images_depth_test, self.labels_test = \
            load_batch_color_depth(
                self.dataset_test, height=self.image_size,
                width=self.image_size, batch_size=batch_size)

        return self.dataset_train


def train(train_class,
          used_structure,
          tfrecord_dir,
          checkpoint_dirs,
          log_dir,
          number_of_steps=None,
          **kwargs):
    train_instance = train_class(used_structure)
    for key in kwargs.copy():
        if hasattr(train_instance, key):
            setattr(train_instance, key, kwargs[key])
            del kwargs[key]
    train_instance.train(
        tfrecord_dir, checkpoint_dirs, log_dir,
        number_of_steps=number_of_steps, **kwargs)
