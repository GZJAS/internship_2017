
%mainfile: Multimodal_learning.tex

\title{Multimodal Learning: Examples in Gesture and Audio-Visual
Speech Recognition\vspace{-0.5em}}
\author{Hsieh Yu-Guan}
\date{\today}
\maketitle

\section*{Abstract}

\section{Introduction}

\section{Related Work}

\section{Presentation of Basic Network Architectures}

\subsection{Convolutional Neural Networks}

Convolutional Neural Networks (CNNs) are an early family of deep learning
architectures inspired from the human vision system \cite{Y. LeCun 1998}.
Generally we have convolutional layers alternating with pooling
(subsamping) layers, but fully connected layers can also be introduced.
CNNs have been shown to achieve state-of-the-art performance in
image processing tasks such as image classification
\cite{A. Krizhevsky 2012} and object detection \cite{Y. LeCun 2010}.
However they can be equally applied in other fields like speech recogntion
\cite{L. Deng 2013}.

\subsection{Auto-encoder}

Auto-encoders are networks that are trained to minimize the reconstruction
error by back-propagating it from the output layer to hidden layers.
In the simplest model with one hidden layer, an auto-encoder takes an
input $\mathbf{x} \in \mathbb{R}^d$ and maps it to the latent
representation $\mathbf{h} \in \mathbb{R}^{d'}$ given by
$\mathbf{h} = \sigma(W\mathbf{x}+\mathbf{b})$ where $W$
is a weight matrix, $\mathbf{b}$ is a bias vector and $\sigma$ is an
activation function. Then the network tries to reconstruct the input
by a reverse mapping $\mathbf{x'} = \sigma(W'\mathbf{h}+\mathbf{b'})$.

To prevent the auto-encoder from learning the identity function as
a trivial solution, several regularization techniques have been proposed.
The bottleneck approach forces dimensionality reduction by having
fewer neurons in hidden layers than in the input layer. For example,
in the above case, we must have $d'<d$. Sparse auto-encoders impose sparsity
on hidden units \cite{A. Makhzani 2014}.
Denoising auto-encoders, which play an important role in my internship,
try to recontruct the clean input from its corrupted version
\cite{P. Vincent 2008, Y. Bengio 2012}. Binomial noise
(switching pixels on or off) adding
to input or hidden layers are used in my case.

Intuitively, auto-encoders are useful for data reconstruction.
Nevertheless, the true interest lies in fact in its capcity to learn
a representation (encoding) for a set of data in a purely unsupervised
fashion \cite{P. Vincent 2010}. Recently, auto-encoders are also
more and more often used as a generative model \cite{Y. Bengio 2013}.

\section{Datasets and Preprocessing}

Many datasets were explored during my internship. The three main datasets
being used are given in details  below. Two of them are for gesture
recognition:
Creative Senz3D \cite{A. Memo 2015, A. Memo 2017} and ASL Finger Spelling
\cite{N. Pugeault 2011}, and one is for AVSR: AVLetters
\cite{I. Matthews 2002}.

\subsection{Creative Senz3D}

The dataset contains gestures perfomed by 4 different people, each
performing 11 different static gestures repeated 30 times each,
for a total of 1320 samples.
For each sample, color, depth and confidence frames are available.
I only used the color and depth frames of this dataset. The original
size of each image is $480 \times 640$ and they're resized to
$299 \times 299$ pixels before being fed to the network. No other
preprocessing are done. For both color and depth images I use the three
color channels (even though a priori only one channel is needed for
depth maps).

\begin{figure}[H]
  \centering
  \hfill
  \includegraphics[width=0.23\linewidth]{dataset/senz3d/examples/1-color}
  \hfill
  \includegraphics[width=0.23\linewidth]{dataset/senz3d/examples/12-color}
  \hfill
  \includegraphics[width=0.23\linewidth]{dataset/senz3d/examples/1-depth}
  \hfill
  \includegraphics[width=0.23\linewidth]{dataset/senz3d/examples/12-depth}
  \caption{%
    \textbf{Example images in the Creative Senz3D dataset.}\\[0.1em]
    Left Two) Color images.\\[0.1em]
    Right Two) Corresponding depth images.\\[0.1em]
    All of the images are of size $480 \times 640$ and contain the
      the entire upper body of the subject.}
  \label{fig:senz3d_exs}
\end{figure}

\subsection{ASL Finger Spelling}

The dataset is composed of more than 60000 images in each modality
(RGB and depth images are provided).
Five subjects are asked to perform the 24 static signs in
the American Sign Language (ASL) alphabet (excluding j and z which involve
motion) a certain number of times, captured with similar lighting
and background.

Images of this dataset are of variable sizes. The data preprocessing
includes resizing each image to $83 \times 83$ pixels,
converting to grayscale and Z-normalization (normailzing to zero mean
and unit of variance).

\begin{figure}[H]
  \centering
  \hfill
  \includegraphics[width=0.23\linewidth]{%
    dataset/fingerspelling5/exs/st/g1}
  \hfill
  \includegraphics[width=0.23\linewidth]{%
    dataset/fingerspelling5/exs/st/g2}
  \hfill
  \includegraphics[width=0.23\linewidth]{%
    dataset/fingerspelling5/exs/st/d1}
  \hfill
  \includegraphics[width=0.23\linewidth]{%
    dataset/fingerspelling5/exs/st/d2}
  \caption{%
    \textbf{Example images in the ASL Finger Spelling dataset
      (after preprocessing).}\\[0.1em]
    Left Two) Grayscale intensity images.\\[0.1em]
    Right Two) The corresponding depth maps.\\[0.1em]
    Images of this dataset have variable sizes, and they're all resized to
      $83 \times 83$ before being fed to the network. Generally only the
      hand region is contained in image.}
  \label{fig:fingerspelling_exs}
\end{figure}

\subsection{AVLetters}

The dataset comprises video and audio recordings of 10 speakers
uttering the letters A to Z, three times each.
We count therefore 780 samples in total. For video data, image sequences
of pre-extracted lip regions are provided.
Each single image if of size $60 \times 80$.
For audio data, only the mel-frequency cepstrum coefficients (MFCCs)
are given, and each audio frame is represented by 26 mfccs.
The lack of raw audio data is a strong constraint on what we're able to do
on this dataset.

Since all utterances don't have the same time duration, I used
fourier resamping to force every video input to be of length 12 and
every audio input to be of length 24. Video frames are Z-normalized.
Several data augmentation techniques
are also considered, including random brightness adjusting, random contrast
adjusting and random cropping (but at least 80\% of the original image
is kept).

\begin{figure}[H]
  \centering
  \includegraphics[width=0.15\linewidth]{%
    dataset/avletters/lips_no_data_aug/1}
  \includegraphics[width=0.15\linewidth]{%
    dataset/avletters/lips_no_data_aug/2}
  \includegraphics[width=0.15\linewidth]{%
    dataset/avletters/lips_no_data_aug/3}
  \includegraphics[width=0.15\linewidth]{%
    dataset/avletters/lips_no_data_aug/4}
  \includegraphics[width=0.15\linewidth]{%
    dataset/avletters/lips_no_data_aug/5}
  \includegraphics[width=0.15\linewidth]{%
    dataset/avletters/lips_no_data_aug/6}\\[0.15em]
  \includegraphics[width=0.15\linewidth]{%
    dataset/avletters/lips_no_data_aug/7}
  \includegraphics[width=0.15\linewidth]{%
    dataset/avletters/lips_no_data_aug/8}
  \includegraphics[width=0.15\linewidth]{%
    dataset/avletters/lips_no_data_aug/9}
  \includegraphics[width=0.15\linewidth]{%
    dataset/avletters/lips_no_data_aug/10}
  \includegraphics[width=0.15\linewidth]{%
    dataset/avletters/lips_no_data_aug/11}
  \includegraphics[width=0.15\linewidth]{%
    dataset/avletters/lips_no_data_aug/12}
  \caption{%
    \textbf{Example visual input for the AVletters dataset
      (left to right, top to bottom).}\\[0.1em]
    Pre-extracted lip regions of $60 \times 80$ pixels are provided.
      Each image sequence is resampled to be of length twelve in order to
      give an input of fixed size to the network.}
  \label{fig:avletters_exs}
\end{figure}

\section{Experimental Setup}

To train a classifier I employed the cross entropy cost function and to
train an auto-encoder the L2 distance between the input and output vector
was used as the loss. For the sake of preventing overfitting, L2
regularization \cite{Y. Bengio 2012} was applied to all the weights of
network with a regularization coefficient $\eta$.
The Adam algorithm \cite{D. Kingma 2014}
was then introduced for minimizing the loss function.
An exponential decay was further used for the stepsize $\alpha$ of this
algorithm with an initial stepsize $\alpha_0$ varying from 0.01 to 0.0001
depending on experiment. The decaying rate $\gamma$
was generally close to 0.8 and
the decay takes place every 100 training steps.

Inputs of the network were normally fed as mini-batches of size 24
(smaller and bigger batch sizes were also experimented).
Batch normalization \cite{S. Ioffe 2015} were introduced after every
convolutional and transposed convoluitonal layer. Therefore, the real
operations used to compute neural activations are more complicated
then what are described above. The used settings and hyperparameter
values aren't necessarily optimal since I wasn't able to test all the
possible combinations.

Here are some more details of the network architectures: ReLu
(Rectified Linear Unit) activation were added to all the hidden layers
\cite{A. Krizhevsky 2012} while no activation function was used for
the output layer.
For classification model dropout \cite{N. Srivastava 2014}
was always applied to the second to last layer during training.
The output of the classification layer was mapped to the probabilities
that one data example belongs to each class by the softmax function.

For classification experiments, except for the one described in
\ref{subsection:AVSR_transfer}, the dataset was always separated into
training and test set. The classifier was first trained on the training data
and then tested on test data once the training phrase was finished.
Unless stated otherwise, the prediction accuracies mentioned hereinafter
were always evaluated on the test set.

\section{Experiments and Results: Unimodal Cases}

\subsection{Classification} \label{subsection:classif}

With every new dataset, I began with training a classifier on it in a
totoally supervised manner.
This gave me an insight into its data quality, the preprocessing
effectiveness and ensured that further experiments could be conducted.
CNN is then one of the most suitable architecture for this purpose.

\subsubsection{Creative Senz3d}

No satisfying results were acquired. It may be due to to a lack of data
quantity, variety, and the fact that the head is also contained in the
image increases significantly the classification difficulty.

\textbf{Subject Dependent.}
In a subject dependant setting, images are separated randomly into
training set (3/4) and test set (1/4). Therefore, during the
testing phase, the classifier doesn't need to deal with data from an
individual that it has never seen before.
In this case, for RGB images, all of the classifiers were able to
have a classification accuracy that is closed to 100\%. This held true
even for a perceptron.
On the other hand, for depth images, the classification accuracy was
between 60\% and 70\% using a perceptron and near 90\% for other CNN
architectures that were tested.

\textbf{Subject Independant.}
On the contrary, the classifier faces individuals never seen before
during testing in a subject independant setting. 
In my case, the training set consisted of images coming from the
first three individuals while the test set contained images of
the final subject.
With the various architectures (including single-layer perceptron
and CNNs varying from three to ten hidden layers)
that I implemented, none of them was able to generalize the learned model
to the new individual.
The pre-trained InceptionV4 architecture \cite{C. Szegedy 2017}
achieved a prediction accuracy
of 30\% for color images and 20\% for depth images (better than chance).

\subsubsection{ASL Finger Spelling} \label{subsubsection:ASL_CNN}

The large number of data contained in this dataset and the relatively
simple image content (single hand instead of the entire upper body)
makes the classification task much easier. By using the CNN architecture
shown in \autoref{fig:CNN10}, I could achieve a classification accuracy
of respectively 80\% and 70\% for intensity and depth images
(\autoref{tab:ASL_classif})
in an subject-independant setting (four subjects for training and one
subject for testing). We may not need that many layers in the CNN
architecture, but further tests were not carried out since it's not
the essential point of my internship.

\begin{figure}[H]
  \centering
  \includegraphics[width=\linewidth]{architectures/CNN10}
  \caption{%
    \textbf{CNN architecture used for the Finger Spelling  dataset.}
      \\[0.1em]
    The input of the nework is a one-channel image of size $83 \times 83$.
      It contains ten hidden layers. S stands for `SAME' padding
      and V stands for `VALID' padding (see text).}
  \label{fig:CNN10}
\end{figure}

\subsubsection{AVletters}

One can refer to \autoref{fig:AVSR_transfer} for the main CNN architectures
that were used in this dataset. 
Notice that 3d CNNs were employed to deal with video inputs.
Considering the small number of available data, a speaker-dependent
setting was used.
With some carefully chosen hyperparameter values and network architectures,
the prediction accracy was of near 80\% for audio and about 55\% for video.

When using CNNs, whether to add the first- and second-order frame-to-frame
differences of MFCCs (\textit{delta} and \textit{delta-delta}) didn't seem
to have a great impact on the final result, so in the latter transfer
learning experiment described in \ref{subsection:AVSR_transfer}, only
MFCCs were fed as input of the network.
Concerning video input, the use of data augmentation techniques only
decreased the learning speed for the training part but didn't improve
the performace for testing.

\subsubsection{Problem of overfitting}

Overfitting was observed for almost all the classification
experiments, and it was particularly severe for
CNNs with many layers.
In fact, CNN architectures could usually classify
perfectly the training data, but experienced a drop of performance
when evaluating on test set.

It's well known that by reducing the number of hidden layers and
increasing the weight regularization coefficient $\eta$ we may
be able to cure this problem.
For example, when $\eta$ was augmented from $0.0004$ to $0.1$,
the classification accuracy for the audio input of the AVLetters
dataset increased by about 10\%.
By using fewer hidden layers in the 3d CNN architecture overfitting
was also alleviated when dealing with the video input of this same dataset.
However, these techniques didn't always work
and more often I got a poorer performance during training without
an improvement of performance for test.

\subsection{Convolutional auto-encoder} \label{subsection:CAE}

Several distinct CAE architectures were tested in my internship.
Here I present the one with five hidden layers; therefore it contains
three convolutional layers and three transposed convolutional layers
as illustrated in \autoref{fig:CAE5}. The end-to-end training instead of
a greedy layer-wise approach was employed.

The proposed architecture was then trained on the two gesture recognition
datasets. First of all, I was interested in the denoising capcity of
the auto-encoder. An example is given in \autoref{fig:image_restoration}.
The auto-encoder is effectively able to reconstruct the clean image in
a way, even though the result is blurred and sometimes distorted.

What's more important, can meaningful high-level features be
learned in this way? The output of the middle layer was taken as a
new representation of the input data.
By doing principal component analysis, it could be projected into
three dimensions for visualization.
However, to quantify the learned features, they're further used for
classification by training a perceptron on top of it.

As suggested in \ref{subsection:classif}, I used a subject-dependant setting
for Creative Senz3d while a subject-independant setting was employed
for ASL Finger Spelling. I compared this new classifier with a
perceptron built on raw data input. As already mentioned earlier, the
latter could classify perfectly the input when dealing with color images
of Creative Senz3d. In other cases, I observed always an improvement
of 10\% to 20\% for the prediction accuracy thanks to the use of the
learned representation of the data (refer to \autoref{tab:ASL_classif}
for results on ASL Finger Spelling). Useful high-level data representation
can thus be learned in a totally unsupervised manner.

\begin{figure}[H]
  \centering
  \includegraphics[width=\linewidth]{architectures/CAE5}
  \caption{%
    \textbf{Convolutional auto-encoder architecture with 
      three convolutional layers and three tranposed convolutional
      layer.}\\[0.1em]
    Activation values of the middle layer are taken as 
      high-level features of the input image. Inputs of the network
      can be of different sizes. We only use valid paddings here.}
  \label{fig:CAE5}
\end{figure}

\begin{figure}[H]
  \centering
  \hfill
  \includegraphics[width=0.28\linewidth]{%
    dataset/fingerspelling5/gray/gray5}
  \hfill
  \includegraphics[width=0.28\linewidth]{%
    dataset/fingerspelling5/gray/gray_dropout5}
  \hfill
  \includegraphics[width=0.28\linewidth]{%
    dataset/fingerspelling5/gray/gray_reconstruction5}
  \caption{%
    \textbf{Image restoration using convolutional auto-encoder.}\\[0.1em]
      Left) Clean Image.\\[0.1em]
      Middle) Noisy image [input].\\[0.1em]
      Right) Restored image [output].}
  \label{fig:image_restoration}
\end{figure}

\section{Experments and Results: Multimodal Cases}

\subsection{Shared representation learning} \label{subsection:shared}

A first fundamental chanllenge in multimodal learning is the problem
of representing and summarizing data from several modalities.
How can we relate information from multiple sources?
Mainly inspired from \cite{J. Ngiam 2011, A. Droniou 2014},
I generalized the CAE architecture introuduced in \ref{subsection:CAE}
to a multimodal setup. As presented in \autoref{fig:FAE5}, two CAEs
of different modalities share a common middle layer by doing a sum
of corresponding values. I first pre-train the first two layers
of each modality using an unimodal CAE. In a second stage, I train the
rest of the network to reconstruct the two modalities of the input.

To prevent the network from finding representations such that different
hidden units are tuned for different modalities separately, random
dropouts are added to inputs in such a way that only by combining the
two modalities we have 100\% of the input information. In particular,
sometimes one modality can be totally absent whereas the whole clean
image is given for the other modality. In addition, proper scaling were
also considered to keep the expected sum of the activations at each layer
to be the same.

\begin{figure}[H]
  \centering
  \includegraphics[width=\linewidth]{architectures/FAE5}
  \caption{%
    \textbf{The bimodal convolutional auto-encoder model that is
      used to learn shared multimodal representation.}\\[0.1em]
    We simply take the CAE architecture that is introduced
      \autoref{fig:CAE5} for each modaliy but force them to have a
      shared middle layer by adding the corresponding activation values.
      We then try to reconstruct the two images separately through
      two disjoint paths.}
  \label{fig:FAE5}
\end{figure}

Ideally, we expect that the network is able to capture corrlations
across different modalities and can thus also learn a better single
modality representation. To verify this hypothesis, I built a perceptron
on the top of the middle layer of the architecture and trained it
in a supervised way while only one modality was given in input. For example,
if I wanted to train a classifier for color images, zeros were fed as
depth inputs. The results are shown in \autoref{tab:ASL_classif}.
Unfortunately, the presence of the two modalities during feature
learning doesn't bring a significant improvement and seems to be useless.

How about exploiting the information from the two modalities in a
totally supervised way? I took the CNN architecture of \autoref{fig:CNN10}
for color and depth images separately until the seventh layer where
a fusion was carried out. After training the network, the prediction
accuracy was always at about 80\% and no improvement was obtained compared
with a CNN trained only on color inputs. To conclude, color and depth
images are probably two modalities that are too similar. Color images
contain already all the necessary information for the task at hand so
that depth maps don't bring any supplementary information that benefit
the specifique purpose I defined.

However, multimodal information may still be useful when one or several
modalities are noisy whereas clean information are available for the others.
For example, I trained a network to construct clean color images from
noisy depth maps. When I did the classification based on the learned
representation, the performance was as if I trained directly a depth CAE.

\begin{table}[H]
  \tabcolsep = 9pt
  \caption{\textbf{Classification performance on the ASL Finger Spelling
    dataset}\\[0.1em]
    Raw) Perceptron that reads raw input data.\\[0.1em]
    CAE features) Perceptron stacked on the middle layer of the CAE.
      (\ref{subsection:CAE}).\\[0.1em]
    Shared) Perceptron that exploits the shared representation learned
      by a bimodal CAE (\ref{subsection:shared}).\\[0.1em]
    CAE architecture) Perceptron stacked on the middle layer of the CAE
      but train the whole network in a supervised way as a CNN.\\[0.1em]
    CNN) The CNN architecture in \autoref{fig:CNN10}
      (\ref{subsubsection:ASL_CNN}). \\[0.1em]
    The used hyperparameters are $\alpha_0=0.005$, $\gamma=0.8$ and
    $\eta=0.0004$. We notice that we have exactly the same network
    architecture for the middle three exerimental setups and only the
    training process differs one from another.
    }
  \label{tab:ASL_classif}
  \begin{tabular*}{\linewidth}{>{\bf}llccccc}
    \toprule
    && Raw & CAE features & Shared & CAE architecture & CNN\\
    \midrule
    \multirow{2}{*}{Intensity} &
    train & 69.47 \% & 78.87 \% & 85.85 \% & 91.29 \% & 99.69 \% \\
    & test & 32.64 \% & 50.24 \% & 53.38 \% & 65.44 \% & 79.73 \% \\
    \midrule
    \multirow{2}{*}{Depth} &
    train & 63.64 \% & 79.61 \% & 81.83 \% & 88.80 \% & 97.24 \% \\
    & test & 29.93 \% & 41.64 \% & 42.85 \% & 55.62 \% & 64.46 \% \\
    \bottomrule
  \end{tabular*}
\end{table}

We may also want to ask if this architecture, when a partial input with
only one modality is given, is able to infer the values of the missing
modality.
Several examples can be seen in \autoref{fig:color_depth_restoration}.
Visually speaking, the reconstruction result seems better when
both modalities are available in input even though they're both very noisy.
Knowing that the two modalities are already quite similar, it reveals
the difficulty of this task and the problem of multimodal retrieval
might be something that is more insteresting than trying to construct
information of some modality from zero.

\begin{figure}[H]
  \centering
  \includegraphics[width=\linewidth]{dataset/senz3d/reconstructions}\\[-1em]
  \caption{%
    \textbf{Restore color and depth images from incomplete input
      information.}\\[0.1em]
    Top) Only the color image is given.\\[0.1em]
    Middle) Only the depth image is given.\\[0.1em]
    Botttom) Both modalities are given but with little information
      (10\% of pixels).}
  \label{fig:color_depth_restoration}
\end{figure}

\subsection{Transfer learning} \label{subsection:AVSR_transfer}

In the second part of this section, we'll discuss the knowledge transfer
problem between different modalities. This work was largely inspired
from \cite{S. Moon 2015}, but at the same time it can also be viewed
as a form of zero-shot learning \cite{A. Frome 2013, R. Socher 2013}.

I studied particularly the transfer between speech and lip-reading
video data using the AVLetters dataset. First the dataset was separated
randomly into two parts, which I'll still call training and test set
by an abus of language. They're respectively noted $Tr$ and $Te$.
$Tr$ contained 600 data samples while $Te$ comprised the left 180 instances.
For $X$ an arbitrary subset of data, $X^a$ and $X^v$ denote respectively
the audio and video data in $X$.

Next, to simulate the imbalance of data quantity between different
modalities in the real world, I further splitted $Tr$, $Te$ into
$Tr_{A\sim T}$, $Tr_{U\sim Z}$, $Te_{A\sim T}$ and $Te_{U\sim Z}$
according to the label of each sample.

\begin{figure}[H]
  \centering
  \includegraphics[width=0.9\linewidth]{architectures/AVSR_transfer}\\[-1em]
  \caption{%
    \textbf{Illustration of the transfer learning approach applied in
      audio and lip-reading speech recognition tasks.}\\[0.1em]
    We first learn two separated model for audio and visual inputs
      (in my cases two CNNs) and try to fine-tune the video network
      with transferred audio data.
    }
  \label{fig:AVSR_transfer}
\end{figure}

\begin{table}[H]
  \tabcolsep = 13pt
  \begin{tabular*}{\linewidth}{>{\bf}lcccccc}
    \toprule
    & $Tr^v$ & $Tr^v_{A\sim T}$ & $Tr^v_{U\sim Z}$
    & $Te^v$ & $Te^v_{A\sim T}$ & $Te^v_{U\sim Z}$\\
    \midrule
    No transfer & 77.67 \% & \textbf{100} \% & 0 \% & \textbf{40.56} \%
    & \textbf{54.48} \% & 0 \% \\
    Exp1 & \textbf{81.17} \% & 98.28 \% & 21.64 \% & 39.44 \%
    & 47.76 \% & 15.22 \% \\
    Exp2 &  40.83 \% & 51.07 \% & 5.22 \% & 23.89 \%
    & 30.60 \% & 4.35 \% \\
    Exp3 & 19.67 \% & 12.23 \% & \textbf{45.52} \% & 12.22 \%
    & 2.24 \% & \textbf{41.34} \% \\
    \bottomrule
  \end{tabular*}
\end{table}
