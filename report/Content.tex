
%mainfile: Multimodal_learning.tex

\title{Multimodal Learning: Examples in Gesture and Audio-Visual
Speech Recognition\vspace{-0.5em}}
\author{Hsieh Yu-Guan}
\date{\today}
\maketitle

\section*{Abstract}

\section{Introduction}

\section{Related Work}

\section{Presentation of Basic Network Architectures}

\section{Datasets and Preprocessing}

Many datasets were explored during my internship. The three main datasets
being used are given in details  below. Two of them are for gesture
recognition:
Creative Senz3D \cite{A. Memo 2015, A. Memo 2017} and ASL Finger Spelling
\cite{N. Pugeault 2011}, and one is for AVSR: Avletters
\cite{I. Matthews 2002}.

\subsection{Creative Senz3D}

The dataset contains gestures perfomed by 4 different people, each
performing 11 different static gestures repeated 30 times each.
For each sample, color, depth and confidence frames are available.
I only used the color and depth frames of this dataset. The original
size of each image is $480 \times 640$ and they're resized to
$299 \times 299$ pixels before being fed to the network. No other
preprocessing are done. For both color and depth images I use the three
color channels (even though a priori only one channel is needed for
depth maps).

\begin{figure}[H]
  \centering
  \hfill
  \includegraphics[width=0.23\linewidth]{dataset/senz3d/examples/1-color}
  \hfill
  \includegraphics[width=0.23\linewidth]{dataset/senz3d/examples/12-color}
  \hfill
  \includegraphics[width=0.23\linewidth]{dataset/senz3d/examples/1-depth}
  \hfill
  \includegraphics[width=0.23\linewidth]{dataset/senz3d/examples/12-depth}
  \caption{%
    \textbf{Example images in the Creative Senz3D dataset.}\\[0.1em]
    Left Two) Color images.\\[0.1em]
    Right Two) Corresponding depth images.\\[0.1em]
    All of the images are of size $480 \times 640$ and contain the
      the entire upper body of the subject.}
  \label{fig:senz3d_exs}
\end{figure}

\subsection{ASL Finger Spelling}

The dataset is composed of more than 60000 images in each modality
(RGB and depth images are given).
Five subjects are asked to perform the 24 static signs in
the American Sign Language (ASL) alphabet (excluding j and z which involve
motion) a certain number of times, captured with similar lighting
and background.

Images of this dataset are of variable sizes. The data preprocessing
includes resizing each image to $83 \times 83$ pixels, and adjusting
contrast of depth maps. Only very late in my internship I added the
Z-normalization (normalize to zero mean and unit of variance)
as a preprocessing step and the only result that was largely changed
is presented in \ref{subsection:CAE}.

\begin{figure}[H]
  \centering
  \hfill
  \includegraphics[width=0.15\linewidth]{%
    dataset/fingerspelling5/exs/or/g1}
  \hfill
  \includegraphics[width=0.15\linewidth]{%
    dataset/fingerspelling5/exs/or/g2}
  \hfill
  \includegraphics[width=0.15\linewidth]{%
    dataset/fingerspelling5/exs/or/d3}
  \hfill
  \includegraphics[width=0.15\linewidth]{%
    dataset/fingerspelling5/exs/or/d4}
  \hfill
  \includegraphics[width=0.15\linewidth]{%
    dataset/fingerspelling5/exs/st/d1}
  \hfill
  \includegraphics[width=0.15\linewidth]{%
    dataset/fingerspelling5/exs/st/d2}
  \caption{%
    \textbf{Example images in the ASL Finger Spelling dataset
      (after preprocessing).}\\[0.1em]
    Left Two) Grayscale intensity images.\\[0.1em]
    Middle Two) Depth maps after adjusting contrast.\\[0.1em]
    Right Two) Depth maps after Z-normalization.\\[0.1em]
    Images of this dataset have variable sizes, and they're all resized to
      $83 \times 83$ before being fed to the network. Generally only the
      hand region is contained in image.}
  \label{fig:fingerspelling_exs}
\end{figure}

\subsection{AVletters}

\begin{figure}[H]
  \centering
  \includegraphics[width=0.15\linewidth]{%
    dataset/avletters/lips_no_data_aug/1}
  \includegraphics[width=0.15\linewidth]{%
    dataset/avletters/lips_no_data_aug/2}
  \includegraphics[width=0.15\linewidth]{%
    dataset/avletters/lips_no_data_aug/3}
  \includegraphics[width=0.15\linewidth]{%
    dataset/avletters/lips_no_data_aug/4}
  \includegraphics[width=0.15\linewidth]{%
    dataset/avletters/lips_no_data_aug/5}
  \includegraphics[width=0.15\linewidth]{%
    dataset/avletters/lips_no_data_aug/6}\\[0.15em]
  \includegraphics[width=0.15\linewidth]{%
    dataset/avletters/lips_no_data_aug/7}
  \includegraphics[width=0.15\linewidth]{%
    dataset/avletters/lips_no_data_aug/8}
  \includegraphics[width=0.15\linewidth]{%
    dataset/avletters/lips_no_data_aug/9}
  \includegraphics[width=0.15\linewidth]{%
    dataset/avletters/lips_no_data_aug/10}
  \includegraphics[width=0.15\linewidth]{%
    dataset/avletters/lips_no_data_aug/11}
  \includegraphics[width=0.15\linewidth]{%
    dataset/avletters/lips_no_data_aug/12}
  \caption{%
    \textbf{Example visual input for the AVletters dataset
      (left to right, top to bottom).}\\[0.1em]
    Pre-extracted lip regions of $60 \times 80$ pixels are provided.
      Each image sequence is resampled to be of length twelve in order to
      give an input of fixed size to the network.}
  \label{fig:avletters_exs}
\end{figure}

\section{Experimental Setup}

\section{Experiences and Results: Unimodal Cases}

\subsection{Classification}

With every new dataset, I began with training a classifier on it in a
totoally supervised manner.
This gave me an insight into its data quality, the preprocessing
effectiveness and ensured that further experiments could be conducted.
CNN is then one of the most suitable architecture for this purpose.

\subsubsection{Creative Senz3d}

No satisfying results were acquired. It may be due to to a lack of data
quantity, variety, and the fact that the head is also contained in the
image increases significantly the classification difficulty.

\textbf{Subject Dependent.}
In a subject dependant setting, images are separated randomly into
training set (3/4) and validation set (1/4). Therefore, during the
validation phase, the classifier doesn't need to deal with data from an
individual that it has never seen before.
In this case, for RGB images, all of the classifiers are able to
have a classification accuracy that is closed to 100\%. This holds true
even for a perceptron.
On the other hand, for depth images, the classification accuracy is
between 60\% and 70\% using a perceptron and near 90\% for other CNN
architectures that were tested.

\textbf{Subject Independant.}
On the contrary, the classifier faces individuals never seen before
during validation in a subject independant setting. 
In my case, the training set consists of images coming from the
first three individuals while the validation set contains images of
the final subject.
With the various architectures (including single-layer perceptron
and CNNs varying from three to ten hidden layers)
that I implemented, none of them is able to generalize the learned model
to the new individual.
The pre-trained InceptionV4 architecture achieves a prediction accuracy
of 30\% for color images and 20\% for depth images (better than chance).

\subsubsection{ASL Finger Spelling}

The large number of data contained in this dataset and the relatively
simple image content (single hand instead of the entire upper body)
makes the classification task much easier. By using the CNN architecture
shown in \autoref{fig:CNN10}, we can achieve a classification accuracy
of respectively 80\% and 70\% for intensity and depth images (Table 1)
in an subject-independant setting (four subjects for training and one
subject for validation). We may not need that many layers in the CNN
architecture, but further tests were not carried out since it's not
the essential point of my internship.

\begin{figure}[H]
  \centering
  \includegraphics[width=\linewidth]{architectures/CNN10}\\[-1.5em]
  \caption{%
    \textbf{CNN architecture used for the Finger Spelling  dataset.}
      \\[0.1em]
    The input of the nework is a one-channel image of size $83 \times 83$.
      It contains ten hidden layers. S stands for `SAME' padding
      and V stands for `VALID' padding (see text).}
  \label{fig:CNN10}
\end{figure}

\subsubsection{AVletters}

One can refer to \autoref{fig:AVSR_transfer} for the main CNN architectures
that are used in this dataset. 
Notice that 3d CNNs are employed to deal with video inputs.
Considering the small number of available data, a speaker-dependant
setting was used, but it didn't save me from the problem of overfitting.
The classification accuracy is of 100\% for training data but only
of 60\% or 55\% for validation data depending on the input modality
(audio then video).

Curiously, for audio data, I get exactly the same classification
performance regardless of the used architecture (perceptron or CNNs)
or the fact that if deltas and delta-deltas are also given in input.
This is not the case when I test with another audio dataset
(not mentioned in the Datasets and Preprocessing section beacause
it wasn't used for main experiences).
For video input, the use of data augmentation techniques only decrease
the learning speed for the training part but doesn't improve the performace
for validation.

\subsection{Convolutional auto-encoder} \label{subsection:CAE}

\begin{figure}[H]
  \centering
  \includegraphics[width=\linewidth]{architectures/CAE5}\\[-2.5em]
  \caption{%
    \textbf{Convolutional auto-encoder architecture with 
      three convolutional layers and three tranposed convolutional
      layer.}\\[0.1em]
    Activation values of the middle layer are taken as 
      high-level features of the input image. Inputs of the network
      can be of different sizes. We only use valid paddings here.}
  \label{fig:CAE5}
\end{figure}

\begin{figure}[H]
  \centering
  \hfill
  \includegraphics[width=0.28\linewidth]{%
    dataset/fingerspelling5/gray/gray4}
  \hfill
  \includegraphics[width=0.28\linewidth]{%
    dataset/fingerspelling5/gray/gray_dropout4}
  \hfill
  \includegraphics[width=0.28\linewidth]{%
    dataset/fingerspelling5/gray/gray_reconstruction4}
  \caption{%
    \textbf{Image restoration using convolutional auto-encoder.}\\[0.1em]
      Left) Clean Image.\\[0.1em]
      Middle) Noisy image [input].\\[0.1em]
      Right) Restored image [output].}
  \label{fig:image_restoration}
\end{figure}

\section{Experiences and Results: Multimodal Cases}

\subsection{Learning shared representation}

\begin{figure}[H]
  \centering
  \includegraphics[width=\linewidth]{architectures/FAE5}\\[-2.5em]
  \caption{%
    \textbf{The bimodal convolutional auto-encoder model that is
      used to learn shared multimodal representation.}\\[0.1em]
    We simply take the CAE architecture that is introduced earlier
      (\autoref{fig:CAE5}) for each modaliy but force them to have a
      shared middle layer by adding the corresponding activation values.
      We then try to reconstruct the two images separately through
      two disjoint paths.}
  \label{fig:FAE5}
\end{figure}

\begin{figure}[H]
  \centering
  \includegraphics[width=\linewidth]{dataset/senz3d/reconstructions}\\[-1em]
  \caption{%
    \textbf{Restore color and depth images from incomplete input
      information.}\\[0.1em]
    Top) Only the color image is given.\\[0.1em]
    Middle) Only the depth image is given.\\[0.1em]
    Botttom) Both modalities are given but with little information
      (10\% of pixels).}
  \label{fig:color_depth_restoration}
\end{figure}

\subsection{Transfer learning}

\begin{figure}[H]
  \centering
  \includegraphics[width=0.9\linewidth]{architectures/AVSR_transfer}\\[-1em]
  \caption{%
    \textbf{Illustration of the transfer learning approach applied in
      audio and lip-reading speech recognition tasks.}\\[0.1em]
    We first learn two separated model for audio and visual inputs
      (in my cases two CNNs) and try to fine-tune the video network
      with transferred audio data.
    }
  \label{fig:AVSR_transfer}
\end{figure}

\newpage

\begin{thebibliography}{99}

\bibitem{M. Asadi-Aghbolaghi 2017}
  17. M. Asadi-Aghbolaghi, A. Clapés, M. Bellantonio, H. J. Escalante, 
  V. Ponce-López, X. Baró, I. Guyon, S. Kasaei, and S. Escalera. 
  A Survey on Deep Learning Based Approaches for Action and Gesture 
  Recognition in Image Sequences. 
  In \textit{Automatic Face \& Gesture Recognition (FG 2017) 2017 12th IEEE
  International Conference}, 2017.

\bibitem{T. Baltrusaitis 2017}
  T. Baltrusaitis, C. Ahuja and L-P. Morency. Multimodal Machine Learning:
  A Survey and Taxonomy. In \textit{CoRR}, vol. abs/1705.09406, 2017.

\bibitem{C. Bregler 1994}
  C. Bregler and Y. Konig. "Eigenlips" for robust speech recognition.
  In \textit{ICASSP}, 1994.

\bibitem{L. Deng 2013}
  L. Deng, G. Hinto and B. Kinsbury. New types of deep neural network
  learning for speech recognition and related applications: An overview.
  In \textit{ICASSP}, 2013.

\bibitem{A. Droniou 2014}
  A. Droniou, S. Ivaldi, and O. Sigaud. A deep unsupervised network
  for multimodal perception, representation and classification. In
  \textit{Robotics and Autonomous System}, 2014.

\bibitem{A. Frome 2013}
  A. Frome, G. S. Corrado, J. Shlens, S. Bengio, J. Dean,
  T. Mikolov, et al. Devise: A deep visual-semantic embedding model. 
  In \textit{NIPS}, 2013.

\bibitem{G. Hinton 2012}
  G. Hinton, L. Deng, D. Yu, G. Dahl, A. Mohamed, N. Jaitly, A. Senior, V.
  Vanhoucke, P. Nguyen, T. Sainath and B. Kingsbury. Deep neural networks
  for acoustic modeling in speech recognition.
  In \textit{IEEE Signal Processing Mag} Vol. 29, 2012.

\bibitem{S. Ioffe 2015}
  S. Ioffe and C. Szegedy. Batch normalization, accelerating deep network
  training by reducing internal covaraiate shift. In \textit{CoRR},
  vol. abs/1502.03167, 2015.

\bibitem{S. Ji 2013}
  S. Ji, W. Xu, M Tang and K. Yu. 3D Convolutional Neural Networks
  for Human Action Recognition. In \textit{IEEE Transactions on Pattern
  Analysis and Machine Intelligence}, vol. 35, 2013.

\bibitem{A. K. Katsuaggelos 2015}
  A. K. Katsaggelos, S. Bahaadini and R. Molina. Audiovisual fusion:
  Challenges and new approaches. In \textit{Proceedings of the IEEE},
  vol. 103, 2015.

\bibitem{A. Krizhevsky 2012}
  A. Krizhevsky, I. Sutskever, and G. E. Hinton.  ImageNet Classification
  with Deep Convolutional Neural Networks. In \textit{NIPS}, 2012.

\bibitem{J. Masci 2011}
  J. Masci, U. Meier, D. C. ̧san, and J. Schmidhuber.
  Stacked convolutional auto-encoders for hierarchical feature extraction.
  In \textit{Artificial Neural Networks and Machine Learning–ICANN}, 2011.

\bibitem{I. Matthews 2002}
  I. Matthews, T. F. Cootes, J. A. Bangham and S. Cox. Extraction of visual
  features for lipreading. In \textit{PAMI}, 2002.

\bibitem{A. Memo 2015}
  A. Memo, L. Minto and P. Zanuttigh.  Exploiting Silhouette Descriptors and
  Synthetic Data for Hand Gesture Recognition. In \textit{STAG: Smart
    Tools \& Apps for Graphics}, 2015.

\bibitem{A. Memo 2017}
  A. Memo and P. Zanuttigh. Head-mounted gesture controlled interface
  for human-computer interaction. In \textit{Multimedia Tools and
  Applications}, 2017.

\bibitem{S. Mitra 2007}
  S. Mitra and T. Acharya. Gesture recognition: A survey. 
  In \textit{IEEE Systems, Man, and Cybernetics}, 37:311–324, 2007. 

\bibitem{P. Molchanov 2015}
  P. Molchanov, S. Gupta, K. Kim, and J. Kautz. Hand gesture recognition 
  with 3D convolutional neural networks. In \textit{CVPRW}, 2015.

\bibitem{S. Moon 2015}
  S. Moon, S. Kim and H. Wang. Multimodal transfer deep learning with
    applications in audio-visual recognition. In \textit{NIPS MMML
    Workshop}, 2015.

\bibitem{J. Nagi 2011}
  J. Nagi, F. Ducatelle, G. Di Caro, D. Ciresan, U. Meier, A. Giusti,
  F. Nagi, J. Schmidhuber, and L. Gambardella.
  Max-pooling convolutional neural networks for vision-based hand 
  gesture recognition.
  In \textit{Proceedings of the IEEE International Conference on
  Signal and Image Processing Applications}, 2011.

\bibitem{N. Neverova 2014}
  N. Neverova, C. Wolf, G. W. Taylor, and F. Nebout.  Multiscale deep
  learning for gesture detection and localization. In \textit{ECCVW}, 2014.

\bibitem{J. Ngiam 2011}
  J. Ngiam, A. Khosla, M. Kim, J. Nam, and A. Y. Ng. 
  Multimodal deep learning. In:
  In \textit{International Conference on Machine Learning}.  28. 
  Bellevue, Washington, USA, 2011.

\bibitem{K. Noda 2014}
  K. Noda, Y. Yamaguchi, K. Nakadai, H. G. Okuno and T. Ogata. Audio-visual
  speech recognition using deep learning.  In \textit{Applied Intelligence,
  vol. 42, no. 4, pp. 722–737, 2014.}

\bibitem{E. Petahan 1988}
  E. Petahan, B. Bischoff, D. Bodoff, and N. M. Brooke. An Improved
  Automatic Lipreading System to enhance Speech Recognition.
  In \textit{ACM SIGCHI}, 1988.

\bibitem{L. Pigou 2014}
 L. Pigou, S. Dieleman, P. J. Kindermans, and B. Schrauwen.
  Sign language recognition using convolutional neural networks. In
  \textit{Workshop at the European Conference on Computer Vision},
  pages 572--578, 2014.

\bibitem{G. Potamianos 2004}
  G. Potamianos, C. Neti, J. Luettin and  I. Matthews. Audio-visual
  automatic speech recognition: An overview. In \textit{Issues in Visual
  and Audio-Visual Speech Processing, MIT Press}, 2004.

\bibitem{N. Pugeault 2011}
  N. Pugeault, and R. Bowden. Spelling It Out: Real-Time ASL
  Fingerspelling Recognition. In \textit{Proceedings of the 1st IEEE
  Workshop on Consumer Depth Cameras for Computer Vision}, 2011.

\bibitem{R. Socher 2013}
  R. Socher, M. Ganjoo, H. Sridhar, O.Bastani, C. D. Manning, and
  A. Y. Ng. Zero-shot learning through cross-modal transfer.
  In \textit{International Conference on Learning Representations (ICLR)},
  Scottsdale, Arizona, USA, 2013.

\bibitem{N. Srivastava 2014}
  N. Srivastava, G. Hinton, A. Krizhevsky, I. Sutskever and R. Salakhutdinov.
  Dropout: A simple way to prevent neural networks from overfitting. In
  \textit{Journal of Machine Learning Research}, vol. 15, 2014.

\bibitem{T. Starner 1998}
  T. Starner, A. Pentland, and J. Weaver. Real-time american sign language
  recognition using desk and wearable computer based video. 
  In \textit{PAMI}, 20(12):1371–1375, 1998.

\bibitem{J. Sung 2015}
  J. Sung, I. Lenz, and A. Saxena. Deep multimodal embedding:
  Manipulating novel objects with point-clouds, language and trajectories.
  In \textit{CoRR}, vol. abs/1509.07831, 2015.

\bibitem{C. Szegedy 2016}
  C. Szegedy, S. Ioffe, V, Vanhoucke. Inception-v4, inception-ResNet and the
  impact of residual connections on learning. In \textit{CoRR},
  vol. abs/1602.07261, 2016.

\bibitem{V. Turchenko 2017}
  V. Turchenko, E. Chalmers and A. Luczak. A deep convolutional auto-encoder
  with pooling - unpooling layers in Caffe. In \textit{CoRR},
  vol. abs/1701.04949, 2017.

\bibitem{J. Weston 2010}
  J. Weston, S. Bengio, and N. Usunier. 
  Large scale image annotation: learning to rank with joint word-image
  embeddings. In \textit{Machine Learning}, 81(1):21--35, 2010.

\bibitem{Di Wu 2016}
  Di Wu, Lionel Pigou, Pieter-Jan Kindermans, Nam Do-Hoang Le, Ling Shao,
  Joni Dambre, and Jean-Marc Odobez. 
  Deep Dynamic Neural Networks for Multimodal Gesture Segmentation and
  Recognition. In \textit{Pattern Analysis and Machine Intelligence
  IEEE Transactions}, vol. 38, 2016.

\bibitem{B. P. Yuhas 1989}
  B. P. Yuhas, M. H. Goldstein, and T. J. Sejnowski. Integration of acoustic
  and visual speech signals using neuralnetworks. in \textit{IEEE Comm.
  Magazine}, pp. 65--71, 1989.

\end{thebibliography}
